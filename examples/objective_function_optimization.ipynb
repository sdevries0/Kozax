{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function optimization\n",
    "\n",
    "In this example, Kozax is used to evolve a symbolic loss function to train a neural network. With each candidate loss function, a neural network is trained on the task of binary classification of XOR data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These device(s) are detected:  [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7), CpuDevice(id=8), CpuDevice(id=9)]\n"
     ]
    }
   ],
   "source": [
    "# Specify the cores to use for XLA\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=10'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax \n",
    "from typing import Callable, Tuple\n",
    "from jax import Array\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/sigur.de.vries/Library/Mobile Documents/com~apple~CloudDocs/phd/kozax\")\n",
    "from kozax.genetic_programming import GeneticProgramming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a fitness function class that includes the network initialization, training loop and weight updates. At every epoch, a new batch of data is sampled, and the fitness is computed as the accuracy of the trained network on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitnessFunction:\n",
    "    \"\"\"\n",
    "    A class to define the fitness function for evaluating candidate loss functions.\n",
    "    The fitness is computed as the accuracy of a neural network trained with the candidate loss function\n",
    "    on a binary classification task (XOR data).\n",
    "\n",
    "    Attributes:\n",
    "        input_dim (int): Dimension of the input data.\n",
    "        hidden_dim (int): Dimension of the hidden layers in the neural network.\n",
    "        output_dim (int): Dimension of the output.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        optim (optax.GradientTransformation): Optax optimizer instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, epochs: int, learning_rate: float):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.optim = optax.adam(learning_rate)\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __call__(self, candidate: str, data: Tuple[Array, Array, Array], tree_evaluator: Callable) -> Array:\n",
    "        \"\"\"\n",
    "        Computes the fitness of a candidate loss function.\n",
    "\n",
    "        Args:\n",
    "            candidate: The candidate loss function (symbolic tree).\n",
    "            data (tuple): A tuple containing the data keys, test keys, and network keys.\n",
    "            tree_evaluator: A function to evaluate the symbolic tree.\n",
    "\n",
    "        Returns:\n",
    "            Array: The mean loss (1 - accuracy) on the validation set.\n",
    "        \"\"\"\n",
    "        data_keys, test_keys, network_keys = data\n",
    "        losses = jax.vmap(self.train, in_axes=[None, 0, 0, 0, None])(candidate, data_keys, test_keys, network_keys, tree_evaluator)\n",
    "        return jnp.mean(losses)\n",
    "\n",
    "    def get_data(self, key: jr.PRNGKey, n_samples: int = 50) -> Tuple[Array, Array]:\n",
    "        \"\"\"\n",
    "        Generates XOR data.\n",
    "\n",
    "        Args:\n",
    "            key (jax.random.PRNGKey): Random key for data generation.\n",
    "            n_samples (int): Number of samples to generate.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input data (x) and the target labels (y).\n",
    "        \"\"\"\n",
    "        x = jr.uniform(key, shape=(n_samples, 2))\n",
    "        y = jnp.logical_xor(x[:,0]>0.5, x[:,1]>0.5)\n",
    "\n",
    "        return x, y[:,None]\n",
    "\n",
    "    def loss_function(self, params: Tuple[Array, Array, Array, Array, Array, Array], x: Array, y: Array, candidate: str, tree_evaluator: Callable) -> Array:\n",
    "        \"\"\"\n",
    "        Computes the loss with an evolved loss function for a given set of parameters and data.\n",
    "\n",
    "        Args:\n",
    "            params (tuple): The parameters of the neural network.\n",
    "            x (Array): The input data.\n",
    "            y (Array): The target labels.\n",
    "            candidate: The candidate loss function (symbolic tree).\n",
    "            tree_evaluator: A function to evaluate the symbolic tree.\n",
    "\n",
    "        Returns:\n",
    "            Array: The mean loss.\n",
    "        \"\"\"\n",
    "        pred = self.neural_network(params, x)\n",
    "        return jnp.mean(jax.vmap(tree_evaluator, in_axes=[None, 0])(candidate, jnp.concatenate([pred, y], axis=-1)))\n",
    "    \n",
    "    def train(self, candidate: str, data_key: jr.PRNGKey, test_key: jr.PRNGKey, network_key: jr.PRNGKey, tree_evaluator: Callable) -> Array:\n",
    "        \"\"\"\n",
    "        Trains a neural network with a given candidate loss function.\n",
    "\n",
    "        Args:\n",
    "            candidate: The candidate loss function (symbolic tree).\n",
    "            data_key (jax.random.PRNGKey): Random key for data generation during training.\n",
    "            test_key (jax.random.PRNGKey): Random key for data generation during testing.\n",
    "            network_key (jax.random.PRNGKey): Random key for initializing the network parameters.\n",
    "            tree_evaluator: A function to evaluate the symbolic tree.\n",
    "\n",
    "        Returns:\n",
    "            Array: The validation loss (1 - accuracy).\n",
    "        \"\"\"\n",
    "        params = self.init_network_params(network_key)\n",
    "\n",
    "        optim_state = self.optim.init(params)\n",
    "\n",
    "        def step(i: int, carry: Tuple[Tuple[Array, Array, Array, Array, Array, Array], optax._src.base.OptState, jr.PRNGKey]) -> Tuple[Tuple[Array, Array, Array, Array, Array, Array], optax._src.base.OptState, jr.PRNGKey]:\n",
    "            params, optim_state, key = carry\n",
    "\n",
    "            key, _key = jr.split(key)\n",
    "\n",
    "            x_train, y_train = self.get_data(_key, n_samples=50)\n",
    "\n",
    "            # Evaluate network parameters and compute gradients\n",
    "            grads = jax.grad(self.loss_function)(params, x_train, y_train, candidate, tree_evaluator)\n",
    "                \n",
    "            # Update parameters\n",
    "            updates, optim_state = self.optim.update(grads, optim_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "            return (params, optim_state, key)\n",
    "\n",
    "        (params, _, _) = jax.lax.fori_loop(0, self.epochs, step, (params, optim_state, data_key))\n",
    "\n",
    "        # Evaluate parameters on test set\n",
    "        x_test, y_test = self.get_data(test_key, n_samples=500)\n",
    "\n",
    "        pred = self.neural_network(params, x_test)\n",
    "        return 1 - jnp.mean(y_test==(pred>0.5)) # Return 1 - accuracy\n",
    "\n",
    "    def neural_network(self, params: Tuple[Array, Array, Array, Array, Array, Array], x: Array) -> Array:\n",
    "        \"\"\"\n",
    "        Defines the neural network architecture (forward pass).\n",
    "\n",
    "        Args:\n",
    "            params (tuple): The parameters of the neural network.\n",
    "            x (Array): The input data.\n",
    "\n",
    "        Returns:\n",
    "            Array: The output of the neural network.\n",
    "        \"\"\"\n",
    "        w1, b1, w2, b2, w3, b3 = params\n",
    "        hidden = jnp.tanh(jnp.dot(x, w1) + b1)\n",
    "        hidden = jnp.tanh(jnp.dot(hidden, w2) + b2)\n",
    "        output = jnp.dot(hidden, w3) + b3\n",
    "        return jax.nn.sigmoid(output)\n",
    "\n",
    "    def init_network_params(self, key: jr.PRNGKey) -> Tuple[Array, Array, Array, Array, Array, Array]:\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the neural network.\n",
    "\n",
    "        Args:\n",
    "            key (jax.random.PRNGKey): Random key for parameter initialization.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the initialized weights and biases.\n",
    "        \"\"\"\n",
    "        key1, key2, key3 = jr.split(key, 3)\n",
    "        w1 = jr.normal(key1, (self.input_dim, self.hidden_dim)) * jnp.sqrt(2.0 / self.input_dim)\n",
    "        b1 = jnp.zeros(self.hidden_dim)\n",
    "        w2 = jr.normal(key2, (self.hidden_dim, self.hidden_dim)) * jnp.sqrt(2.0 / self.hidden_dim)\n",
    "        b2 = jnp.zeros(self.hidden_dim)\n",
    "        w3 = jr.normal(key3, (self.hidden_dim, self.output_dim)) * jnp.sqrt(2.0 / self.hidden_dim)\n",
    "        b3 = jnp.zeros(self.output_dim)\n",
    "        return (w1, b1, w2, b2, w3, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the optimized loss function generalizes, a batch of neural networks are trained with different data and weight initialization. For this purpose, a batch of keys for initialization, data sampling and validation data are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keys(key, batch_size=4):\n",
    "    key1, key2, key3 = jr.split(key, 3)\n",
    "    return jr.split(key1, batch_size), jr.split(key2, batch_size), jr.split(key3, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the hyperparameters and inputs to the genetic programming algorithm. The inputs to the trees are the prediction and target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data should be formatted as: ['pred', 'y'].\n",
      "In generation 1, best fitness = 0.4820, best solution = log(pred*y + pred - y)\n",
      "In generation 2, best fitness = 0.4560, best solution = y*(y - 0.257)*log(pred*y + pred - y)\n",
      "In generation 3, best fitness = 0.4560, best solution = y*(y - 0.257)*log(pred*y + pred - y)\n",
      "In generation 4, best fitness = 0.4560, best solution = y*(y - 0.257)*log(pred*y + pred - y)\n",
      "In generation 5, best fitness = 0.4560, best solution = y*(y - 0.257)*log(pred*y + pred - y)\n",
      "In generation 6, best fitness = 0.3335, best solution = 2*pred*(0.425 - y)\n",
      "In generation 7, best fitness = 0.3335, best solution = 2*pred*(0.425 - y)\n",
      "In generation 8, best fitness = 0.3335, best solution = pred*(0.425 - y)\n",
      "In generation 9, best fitness = 0.3335, best solution = pred*(0.425 - y)\n",
      "In generation 10, best fitness = 0.3300, best solution = (0.425 - y)*log(pred + 0.793)\n",
      "In generation 11, best fitness = 0.1045, best solution = 3*pred*(pred - y - 0.597)\n",
      "In generation 12, best fitness = 0.1045, best solution = 3*pred*(pred - y - 0.597)\n",
      "In generation 13, best fitness = 0.1045, best solution = 3*pred*(pred - y - 0.597)\n",
      "In generation 14, best fitness = 0.0955, best solution = (2*pred + 0.107)*(pred - y - 0.597)\n",
      "In generation 15, best fitness = 0.0955, best solution = (2*pred + 0.107)*(pred - y - 0.597)\n",
      "In generation 16, best fitness = 0.0915, best solution = (pred - 2*y + 0.107)*log(pred + 0.793)\n",
      "In generation 17, best fitness = 0.0915, best solution = (pred - 2*y + 0.107)*log(pred + 0.793)\n",
      "In generation 18, best fitness = 0.0915, best solution = (pred - 2*y + 0.107)*log(pred + 0.793)\n",
      "In generation 19, best fitness = 0.0915, best solution = (pred - 2*y + 0.107)*log(pred + 0.793)\n",
      "In generation 20, best fitness = 0.0895, best solution = (pred + 0.107)*(pred - 2*y - 0.103)\n",
      "In generation 21, best fitness = 0.0865, best solution = pred*(pred - y - log(pred + 0.793) + 0.107)\n",
      "In generation 22, best fitness = 0.0850, best solution = (pred - 0.211)*(pred - y - log(pred + 0.793) + 0.107)\n",
      "In generation 23, best fitness = 0.0850, best solution = (pred - 0.211)*(pred - y - log(pred + 0.793) + 0.107)\n",
      "In generation 24, best fitness = 0.0850, best solution = (pred - 0.211)*(pred - y - log(pred + 0.793) + 0.107)\n",
      "In generation 25, best fitness = 0.0850, best solution = (pred - 0.211)*(pred - y - log(pred + 0.793) + 0.107)\n",
      "Complexity: 1, fitness: 0.4909999668598175, equations: nan\n",
      "Complexity: 4, fitness: 0.4714999794960022, equations: log(0.25 - pred)\n",
      "Complexity: 6, fitness: 0.4139999747276306, equations: log(-pred + y - 0.496)\n",
      "Complexity: 7, fitness: 0.119999960064888, equations: pred*(pred - y - 0.354)\n",
      "Complexity: 9, fitness: 0.09499996155500412, equations: pred*(pred - y - 0.457)\n",
      "Complexity: 11, fitness: 0.09149995446205139, equations: (pred - 0.211)*(pred - 2*y + 0.107)\n",
      "Complexity: 12, fitness: 0.08649995177984238, equations: pred*(pred - y - log(pred + 0.793) + 0.107)\n",
      "Complexity: 14, fitness: 0.0849999487400055, equations: (pred - 0.211)*(pred - y - log(pred + 0.793) + 0.107)\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "data_key, gp_key = jr.split(key)\n",
    "\n",
    "population_size = 50\n",
    "num_populations = 5\n",
    "num_generations = 25\n",
    "\n",
    "operator_list = [(\"+\", lambda x, y: jnp.add(x, y), 2, 0.5), \n",
    "                (\"-\", lambda x, y: jnp.subtract(x, y), 2, 0.5),\n",
    "                (\"*\", lambda x, y: jnp.multiply(x, y), 2, 0.5),\n",
    "                (\"log\", lambda x: jnp.log(x + 1e-7), 1, 0.1),\n",
    "                ]\n",
    "\n",
    "variable_list = [[\"pred\", \"y\"]]\n",
    "\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "fitness_function = FitnessFunction(input_dim, hidden_dim, output_dim, learning_rate=0.01, epochs=100)\n",
    "\n",
    "strategy = GeneticProgramming(num_generations, population_size, fitness_function, operator_list, variable_list, num_populations = num_populations)\n",
    "\n",
    "data_keys, test_keys, network_keys = generate_keys(data_key)\n",
    "\n",
    "strategy.fit(gp_key, (data_keys, test_keys, network_keys), verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
